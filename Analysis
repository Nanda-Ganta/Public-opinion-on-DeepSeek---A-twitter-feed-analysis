# -*- coding: utf-8 -*-
"""Topic modelling

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NDmDKMm6vc1JXGq8ECh_zxLj2NwUnVpL
"""

import warnings
import logging

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore")
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

# Install compatible package versions
!pip uninstall -y numpy scipy gensim
!pip install numpy==1.24.3 scipy==1.10.1 gensim==4.3.2 nltk scikit-learn pandas matplotlib wordcloud

import warnings
import logging
import numpy as np
import re
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from gensim import corpora
from gensim.models import LdaMulticore
from gensim.models import CoherenceModel
from sklearn.model_selection import ParameterGrid
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from google.colab import drive
from collections import Counter

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore")
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

# Download NLTK data
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('wordnet')

# Mount Google Drive
drive.mount('/content/drive/')

# Load dataset
df = pd.read_csv('/content/drive/My Drive/BPA/for_export_deepseek.csv')

# Rename columns to snake case
column_name_mapping = {
    'pseudo_id': 'tweet_id', 'text': 'tweet_text', 'retweetCount': 'retweet_count',
    'replyCount': 'reply_count', 'likeCount': 'like_count', 'quoteCount': 'quote_count',
    'viewCount': 'view_count', 'bookmarkCount': 'bookmark_count', 'createdAt': 'created_at',
    'lang': 'language', 'isReply': 'is_reply', 'pseudo_inReplyToId': 'in_reply_to_tweet_id',
    'pseudo_conversationId': 'conversation_id', 'pseudo_inReplyToUserId': 'in_reply_to_user_id',
    'pseudo_author_id': 'author_id', 'author_location': 'author_location',
    'author_followers': 'author_followers_count', 'author_following': 'author_following_count',
    'author_isVerified': 'author_is_verified', 'author_isBlueVerified': 'author_is_blue_verified'
}
df = df.rename(columns=column_name_mapping)

# Map language codes to names
language_mapping = {
    'en': 'English', 'ja': 'Japanese', 'uk': 'Ukrainian', 'nl': 'Dutch', 'hi': 'Hindi',
    'it': 'Italian', 'es': 'Spanish', 'in': 'Indonesian', 'de': 'German', 'tr': 'Turkish',
    'ca': 'Catalan', 'zh': 'Chinese', 'fr': 'French', 'ko': 'Korean', 'th': 'Thai',
    'und': 'artificial_undetermined_language', 'gu': 'Gujarati', 'lt': 'Lithuanian',
    'ru': 'Russian', 'fa': 'Persian', 'ro': 'Romanian', 'cs': 'Czech', 'ar': 'Arabic',
    'pt': 'Portuguese', 'tl': 'Filipino', 'mr': 'Marathi', 'fi': 'Finnish', 'et': 'Estonian',
    'ur': 'Urdu', 'hu': 'Hungarian', 'pl': 'Polish', 'ta': 'Tamil', 'sv': 'Swedish',
    'te': 'Telugu', 'ml': 'Malayalam', 'bn': 'Bengali', 'ht': 'Haitian Creole', 'el': 'Greek',
    'cy': 'Welsh', 'vi': 'Vietnamese', 'ne': 'Nepali', 'kn': 'Kannada', 'lv': 'Latvian',
    'bg': 'Bulgarian', 'eu': 'Basque', 'sl': 'Slovenian', 'no': 'Norwegian', 'iw': 'Hebrew',
    'da': 'Danish', 'si': 'Sinhala', 'is': 'Icelandic', 'ps': 'Pashto', 'my': 'Burmese',
    'pa': 'Punjabi', 'ckb': 'Sorani Kurdish', 'sr': 'Serbian', 'am': 'Amharic',
    'hy': 'Armenian', 'or': 'Odia', 'sd': 'Sindhi', 'lo': 'Lao', 'ka': 'Georgian',
    'dv': 'Dhivehi', 'zxx': 'artificial_media', 'qam': 'artificial_mentions',
    'qme': 'artificial_media links', 'qht': 'artificial_hashtags', 'qct': 'artificial_cash_tags',
    'qst': 'artificial_very_short_text'
}
df['language'] = df['language'].map(language_mapping)

# Fill missing author_location with 'Unknown'
df['author_location'] = df['author_location'].fillna('Unknown')


# Filter for English tweets
english_df = df[df['language'] == 'English']

# Use 100% of the english tweets
sampled_df = english_df
print(f"Using {len(sampled_df)} rows (100%) from English tweets.")

sampled_df.sort_values('like_count', ascending=False).head(int(len(sampled_df) * 0.0001)).describe()

# Preprocess text
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    text = re.sub(r'http\S+|www\S+|https\S+|@\w+|#\w+|[^A-Za-z\s]', '', text)
    tokens = word_tokenize(text.lower())
    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and len(token) > 2]
    return tokens

sampled_df['processed_text'] = sampled_df['tweet_text'].apply(preprocess_text)

# Display word counts
all_words = [word for text in sampled_df['processed_text'] for word in text]
word_counts = Counter(all_words)
print("\nTop 10 most common words:")
for word, count in word_counts.most_common(10):
    print(f"{word}: {count}")

# Create dictionary and corpus
dictionary = corpora.Dictionary(sampled_df['processed_text'])
dictionary.filter_extremes(no_below=5, no_above=0.5)
corpus = [dictionary.doc2bow(text) for text in sampled_df['processed_text']]

# Define parameter grid
param_grid = {
    'num_topics': [3, 5, 10],
    'passes': [10, 15, 20]
}

# Function to compute coherence score
def compute_coherence_score(lda_model, texts, dictionary):
    coherence_model = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')
    return coherence_model.get_coherence()

# Function to calculate topic diversity
def calculate_topic_diversity(lda_model, num_topics):
    topic_words = [lda_model.show_topic(i, topn=10) for i in range(num_topics)]
    topic_words_flat = [word for topic in topic_words for word, _ in topic]
    topic_words_set = set(topic_words_flat)
    return len(topic_words_set) / len(topic_words_flat)

# Function to evaluate topic stability
def evaluate_topic_stability(lda_model, corpus, dictionary, num_runs=3):
    topic_distributions = []
    for _ in range(num_runs):
        temp_model = LdaMulticore(
            corpus=corpus, id2word=dictionary, num_topics=lda_model.num_topics,
            passes=lda_model.passes, workers=2, random_state=np.random.randint(1000)
        )
        topic_distributions.append([temp_model.get_topic_terms(i, topn=10) for i in range(temp_model.num_topics)])
    stable_topics = []
    for i in range(lda_model.num_topics):
        top_words = [set([dictionary[word_id] for word_id, _ in topic[i]]) for topic in topic_distributions]
        common_words = set.intersection(*top_words)
        stability_score = len(common_words) / 10  # Assuming topn=10
        stable_topics.append(stability_score)
    return np.mean(stable_topics)

# Hyperparameter tuning
grid = ParameterGrid(param_grid)
results = []

for params in grid:
    # Train LDA model
    lda_model = LdaMulticore(
        corpus=corpus, id2word=dictionary, num_topics=params['num_topics'],
        passes=params['passes'], workers=2, random_state=42
    )

    # Evaluate model
    coherence_score = compute_coherence_score(lda_model, sampled_df['processed_text'], dictionary)
    topic_diversity = calculate_topic_diversity(lda_model, params['num_topics'])
    stability_score = evaluate_topic_stability(lda_model, corpus, dictionary)

    # Store results
    results.append({
        **params,
        'coherence_score': coherence_score,
        'topic_diversity': topic_diversity,
        'stability_score': stability_score
    })
